{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cf6f890-1e5f-41c0-936e-9e4c23d54fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.00\n",
      "Validation Accuracy: 0.91\n",
      "Testing Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "#1.Implement a perceptron from scratch \n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Load training data\n",
    "training_path = \"/home/manichandana-sandhaboina/Downloads/train\"\n",
    "images = []\n",
    "labels = []\n",
    "for root, dirs, files in os.walk(training_path):\n",
    "    for filename in files:\n",
    "        filepath = os.path.join(root, filename)\n",
    "        label = os.path.basename(root)\n",
    "        image = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (28, 28))\n",
    "            images.append(image.flatten())\n",
    "            labels.append(label)\n",
    "X_train = np.array(images)\n",
    "y_train = np.array(labels)\n",
    "\n",
    "# Map labels to integers\n",
    "label_to_int = {label: idx for idx, label in enumerate(np.unique(y_train))}\n",
    "y_train = np.array([label_to_int[label] for label in y_train])\n",
    "\n",
    "# Initialize perceptron\n",
    "input_size = X_train.shape[1]\n",
    "weights = np.zeros(input_size)\n",
    "bias = 0\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Train perceptron\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X_train)):\n",
    "        prediction = np.dot(X_train[i], weights) + bias\n",
    "        prediction = 1 if prediction >= 0 else 0\n",
    "        if prediction != y_train[i]:\n",
    "            update = learning_rate * (y_train[i] - prediction)\n",
    "            weights += update * X_train[i]\n",
    "            bias += update\n",
    "\n",
    "# Evaluate training accuracy\n",
    "correct = 0\n",
    "for i in range(len(X_train)):\n",
    "    prediction = np.dot(X_train[i], weights) + bias\n",
    "    prediction = 1 if prediction >= 0 else 0\n",
    "    if prediction == y_train[i]:\n",
    "        correct += 1\n",
    "train_acc = correct / len(X_train)\n",
    "print(f\"Training Accuracy: {train_acc:.2f}\")\n",
    "\n",
    "# Optionally, load validation data and evaluate accuracy\n",
    "validation_path = \"/home/manichandana-sandhaboina/Downloads/validation\"\n",
    "images = []\n",
    "labels = []\n",
    "for root, dirs, files in os.walk(validation_path):\n",
    "    for filename in files:\n",
    "        filepath = os.path.join(root, filename)\n",
    "        label = os.path.basename(root)\n",
    "        image = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (28, 28))\n",
    "            images.append(image.flatten())\n",
    "            labels.append(label)\n",
    "X_val = np.array(images)\n",
    "y_val = np.array(labels)\n",
    "\n",
    "label_to_int = {label: idx for idx, label in enumerate(np.unique(y_val))}\n",
    "y_val = np.array([label_to_int[label] for label in y_val])\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(X_val)):\n",
    "    prediction = np.dot(X_val[i], weights) + bias\n",
    "    prediction = 1 if prediction >= 0 else 0\n",
    "    if prediction == y_val[i]:\n",
    "        correct += 1\n",
    "val_acc = correct / len(X_val)\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}\")\n",
    "\n",
    "# Optionally, load testing data and evaluate accuracy\n",
    "testing_path = \"/home/manichandana-sandhaboina/Downloads/test\"\n",
    "images = []\n",
    "labels = []\n",
    "for root, dirs, files in os.walk(testing_path):\n",
    "    for filename in files:\n",
    "        filepath = os.path.join(root, filename)\n",
    "        label = os.path.basename(root)\n",
    "        image = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (28, 28))\n",
    "            images.append(image.flatten())\n",
    "            labels.append(label)\n",
    "X_test = np.array(images)\n",
    "y_test = np.array(labels)\n",
    "\n",
    "label_to_int = {label: idx for idx, label in enumerate(np.unique(y_test))}\n",
    "y_test = np.array([label_to_int[label] for label in y_test])\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(X_test)):\n",
    "    prediction = np.dot(X_test[i], weights) + bias\n",
    "    prediction = 1 if prediction >= 0 else 0\n",
    "    if prediction == y_test[i]:\n",
    "        correct += 1\n",
    "test_acc = correct / len(X_test)\n",
    "print(f\"Testing Accuracy: {test_acc:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0fdd7e-3985-4e19-8378-31907789f6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping /home/manichandana-sandhaboina/Downloads/train/MNIST/raw/t10k-images-idx3-ubyte: cannot identify image file '/home/manichandana-sandhaboina/Downloads/train/MNIST/raw/t10k-images-idx3-ubyte'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/train/MNIST/raw/t10k-images-idx3-ubyte.gz: cannot identify image file '/home/manichandana-sandhaboina/Downloads/train/MNIST/raw/t10k-images-idx3-ubyte.gz'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/train/MNIST/raw/t10k-labels-idx1-ubyte: cannot identify image file '/home/manichandana-sandhaboina/Downloads/train/MNIST/raw/t10k-labels-idx1-ubyte'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/train/MNIST/raw/t10k-labels-idx1-ubyte.gz: cannot identify image file '/home/manichandana-sandhaboina/Downloads/train/MNIST/raw/t10k-labels-idx1-ubyte.gz'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/train/MNIST/raw/train-images-idx3-ubyte: cannot identify image file '/home/manichandana-sandhaboina/Downloads/train/MNIST/raw/train-images-idx3-ubyte'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/train/MNIST/raw/train-images-idx3-ubyte.gz: cannot identify image file '/home/manichandana-sandhaboina/Downloads/train/MNIST/raw/train-images-idx3-ubyte.gz'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/train/MNIST/raw/train-labels-idx1-ubyte: cannot identify image file '/home/manichandana-sandhaboina/Downloads/train/MNIST/raw/train-labels-idx1-ubyte'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/train/MNIST/raw/train-labels-idx1-ubyte.gz: cannot identify image file '/home/manichandana-sandhaboina/Downloads/train/MNIST/raw/train-labels-idx1-ubyte.gz'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/t10k-images-idx3-ubyte: cannot identify image file '/home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/t10k-images-idx3-ubyte'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/t10k-images-idx3-ubyte.gz: cannot identify image file '/home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/t10k-images-idx3-ubyte.gz'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/t10k-labels-idx1-ubyte: cannot identify image file '/home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/t10k-labels-idx1-ubyte'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/t10k-labels-idx1-ubyte.gz: cannot identify image file '/home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/t10k-labels-idx1-ubyte.gz'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/train-images-idx3-ubyte: cannot identify image file '/home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/train-images-idx3-ubyte'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/train-images-idx3-ubyte.gz: cannot identify image file '/home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/train-images-idx3-ubyte.gz'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/train-labels-idx1-ubyte: cannot identify image file '/home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/train-labels-idx1-ubyte'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/train-labels-idx1-ubyte.gz: cannot identify image file '/home/manichandana-sandhaboina/Downloads/validation/MNIST/raw/train-labels-idx1-ubyte.gz'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/test/MNIST/raw/t10k-images-idx3-ubyte: cannot identify image file '/home/manichandana-sandhaboina/Downloads/test/MNIST/raw/t10k-images-idx3-ubyte'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/test/MNIST/raw/t10k-images-idx3-ubyte.gz: cannot identify image file '/home/manichandana-sandhaboina/Downloads/test/MNIST/raw/t10k-images-idx3-ubyte.gz'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/test/MNIST/raw/t10k-labels-idx1-ubyte: cannot identify image file '/home/manichandana-sandhaboina/Downloads/test/MNIST/raw/t10k-labels-idx1-ubyte'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/test/MNIST/raw/t10k-labels-idx1-ubyte.gz: cannot identify image file '/home/manichandana-sandhaboina/Downloads/test/MNIST/raw/t10k-labels-idx1-ubyte.gz'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/test/MNIST/raw/train-images-idx3-ubyte: cannot identify image file '/home/manichandana-sandhaboina/Downloads/test/MNIST/raw/train-images-idx3-ubyte'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/test/MNIST/raw/train-images-idx3-ubyte.gz: cannot identify image file '/home/manichandana-sandhaboina/Downloads/test/MNIST/raw/train-images-idx3-ubyte.gz'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/test/MNIST/raw/train-labels-idx1-ubyte: cannot identify image file '/home/manichandana-sandhaboina/Downloads/test/MNIST/raw/train-labels-idx1-ubyte'\n",
      "Skipping /home/manichandana-sandhaboina/Downloads/test/MNIST/raw/train-labels-idx1-ubyte.gz: cannot identify image file '/home/manichandana-sandhaboina/Downloads/test/MNIST/raw/train-labels-idx1-ubyte.gz'\n",
      "Epoch [1/10], Loss: 1.0781, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [2/10], Loss: 0.0388, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [3/10], Loss: 0.0015, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Testing Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#2.Build and train a simple neural network using a framework like TensorFlow or PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "training_path = \"/home/manichandana-sandhaboina/Downloads/train\"\n",
    "validation_path = \"/home/manichandana-sandhaboina/Downloads/validation\"\n",
    "testing_path = \"/home/manichandana-sandhaboina/Downloads/test\"\n",
    "\n",
    "root = training_path\n",
    "transform = transform\n",
    "samples = []\n",
    "for root, _, fnames in sorted(os.walk(root)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            samples.append((path, img))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "\n",
    "train_loader = DataLoader([(transform(sample[1]), 0) for sample in samples], batch_size=32, shuffle=True)\n",
    "\n",
    "root = validation_path\n",
    "transform = transform\n",
    "samples = []\n",
    "for root, _, fnames in sorted(os.walk(root)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            samples.append((path, img))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "\n",
    "val_loader = DataLoader([(transform(sample[1]), 0) for sample in samples], batch_size=32, shuffle=False)\n",
    "\n",
    "root = testing_path\n",
    "transform = transform\n",
    "samples = []\n",
    "for root, _, fnames in sorted(os.walk(root)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            samples.append((path, img))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "\n",
    "test_loader = DataLoader([(transform(sample[1]), 0) for sample in samples], batch_size=32, shuffle=False)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "test_acc = correct / total\n",
    "print(f\"Testing Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a16fee46-c30c-41c0-ae6e-d1f832cd73da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.43349777888844593\n",
      "Epoch 2, Loss: 0.17687799556333902\n",
      "Epoch 3, Loss: 0.12952829222319093\n",
      "Epoch 4, Loss: 0.10619127004344728\n",
      "Epoch 5, Loss: 0.08731912212991011\n",
      "Epoch 6, Loss: 0.07421592731707657\n",
      "Epoch 7, Loss: 0.06527332497736799\n",
      "Epoch 8, Loss: 0.05884556728986707\n",
      "Epoch 9, Loss: 0.05234267381593891\n",
      "Epoch 10, Loss: 0.04615782773017331\n",
      "Epoch 11, Loss: 0.04149583718455432\n",
      "Epoch 12, Loss: 0.03548893290002837\n",
      "Epoch 13, Loss: 0.033439223111193156\n",
      "Epoch 14, Loss: 0.029517166675539316\n",
      "Epoch 15, Loss: 0.025470443684500896\n",
      "Epoch 16, Loss: 0.02378686530941828\n",
      "Epoch 17, Loss: 0.025655421347896484\n",
      "Epoch 18, Loss: 0.020200532132992274\n",
      "Epoch 19, Loss: 0.020018791839263722\n",
      "Epoch 20, Loss: 0.018038608176792467\n",
      "Training Accuracy: 99.14333333333333 %\n",
      "Test Accuracy: 97.67 %\n",
      "Validation Accuracy: 97.67 %\n"
     ]
    }
   ],
   "source": [
    "#3.Create a multi-layer perceptron (MLP) for digit classification (MNIST dataset)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transforms for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load data from folders\n",
    "train_data = datasets.MNIST(\"/home/manichandana-sandhaboina/Downloads/train\", download=True, train=True, transform=transform)\n",
    "validation_data = datasets.MNIST(\"/home/manichandana-sandhaboina/Downloads/validation\", download=True, train=False, transform=transform)\n",
    "test_data = datasets.MNIST(\"/home/manichandana-sandhaboina/Downloads/test\", download=True, train=False, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Multi-layer perceptron (MLP) model\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/i}')\n",
    "\n",
    "# Evaluate on training set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Training Accuracy: {100 * correct / total} %')\n",
    "\n",
    "# Evaluate on test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Test Accuracy: {100 * correct / total} %')\n",
    "\n",
    "# Evaluate on validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Validation Accuracy: {100 * correct / total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d413d915-bfa6-4799-afe5-4825cec8754f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with L1 regularization...\n",
      "Epoch 1, Loss: 2.4368600845336914, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 0.6417902708053589, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.6720300912857056, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.7013783057530721, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.7260257403055826, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 6, Loss: 0.745365560054779, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 7, Loss: 0.7600256204605103, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 8, Loss: 0.7708768844604492, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 9, Loss: 0.7788434227307638, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 10, Loss: 0.7845848401387533, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with L1 regularization: 100.0%\n",
      "Training with L2 regularization...\n",
      "Epoch 1, Loss: 1.6748311718304951, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 0.6464554866154989, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.6812806725502014, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.7131774624188741, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.739063024520874, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 6, Loss: 0.7589763402938843, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 7, Loss: 0.7738411625226339, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 8, Loss: 0.7847724556922913, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 9, Loss: 0.7926784952481588, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 10, Loss: 0.7983372608820597, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with L2 regularization: 100.0%\n",
      "Training with Dropout regularization...\n",
      "Epoch 1, Loss: 0.9202661663293839, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 2.4835262735223296e-09, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 6, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 7, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 8, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 9, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 10, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with Dropout regularization: 100.0%\n"
     ]
    }
   ],
   "source": [
    "#4Experiment with different regularization techniques on a neural network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "\n",
    "# Define paths to datasets\n",
    "training_path = \"/home/manichandana-sandhaboina/Downloads/train\"\n",
    "validation_path = \"/home/manichandana-sandhaboina/Downloads/validation\"\n",
    "testing_path = \"/home/manichandana-sandhaboina/Downloads/test\"\n",
    "\n",
    "# Define transforms for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Paths to datasets\n",
    "datasets = [\n",
    "    {'name': 'train', 'path': training_path},\n",
    "    {'name': 'validation', 'path': validation_path},\n",
    "    {'name': 'test', 'path': testing_path}\n",
    "]\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "data_loaders = {}\n",
    "for dataset in datasets:\n",
    "    images = [f for f in os.listdir(dataset['path']) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    data = []\n",
    "    labels = []\n",
    "    for img_name in images:\n",
    "        img_path = os.path.join(dataset['path'], img_name)\n",
    "        image = Image.open(img_path)\n",
    "        image = transform(image)\n",
    "        data.append(image)\n",
    "        labels.append(torch.tensor(0))  # Assuming all labels are 0 for simplicity\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    data_tensor = torch.stack(data)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "\n",
    "    # Create TensorDataset and DataLoader\n",
    "    data_loaders[dataset['name']] = DataLoader(TensorDataset(data_tensor, labels_tensor), batch_size=64, shuffle=dataset['name'] == 'train')\n",
    "\n",
    "# Neural network model\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(224 * 224 * 3, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Regularization techniques\n",
    "regularization_techniques = ['L1', 'L2', 'Dropout']\n",
    "\n",
    "for technique in regularization_techniques:\n",
    "    print(f\"Training with {technique} regularization...\")\n",
    "    \n",
    "    # Reset model parameters and optimizer\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Dropout):\n",
    "            module.p = 0.5\n",
    "            \n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    # Train the network\n",
    "    num_epochs = 10 # Increase number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        net.train()  # Set the model to train mode\n",
    "        for dataset_name in ['train']:\n",
    "            for i, (inputs, labels) in enumerate(data_loaders[dataset_name], 0):\n",
    "                optimizer.zero_grad()\n",
    "                inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Apply regularization for L1 and L2\n",
    "                if technique == 'L1' or technique == 'L2':\n",
    "                    l1_lambda = 0.00001  # Adjust regularization strength\n",
    "                    l2_lambda = 0.0001   # Adjust regularization strength\n",
    "                    l1_reg = torch.tensor(0., requires_grad=False)\n",
    "                    l2_reg = torch.tensor(0., requires_grad=False)\n",
    "                    for param in net.parameters():\n",
    "                        l1_reg += torch.norm(param, 1)\n",
    "                        l2_reg += torch.norm(param, 2)\n",
    "                    loss += l1_lambda * l1_reg + l2_lambda * l2_reg\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loaders['train']:\n",
    "                inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            train_accuracy = 100 * correct / total\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in data_loaders['validation']:\n",
    "                inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            validation_accuracy = 100 * correct / total\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Loss: {running_loss / len(data_loaders[\"train\"])}, Train Accuracy: {train_accuracy}%, Validation Accuracy: {validation_accuracy}%')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loaders['test']:\n",
    "            inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        test_accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy with {technique} regularization: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a0ae910-cd3b-4f3d-80df-7f0fe873fada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD optimizer...\n",
      "Epoch 1, Loss: 0.8833242257436117, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 6, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 7, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 8, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 9, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 10, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with SGD optimizer: 100.0%\n",
      "Training with Adam optimizer...\n",
      "Epoch 1, Loss: 6.733063141504924, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 0.2000760038693746, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 6, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 7, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 8, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 9, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 10, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with Adam optimizer: 100.0%\n",
      "Training with RMSprop optimizer...\n",
      "Epoch 1, Loss: 0.8549737930297852, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 6, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 7, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 8, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 9, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 10, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with RMSprop optimizer: 100.0%\n",
      "Training with Adagrad optimizer...\n",
      "Epoch 1, Loss: 0.9245852629343668, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 6, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 7, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 8, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 9, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 10, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with Adagrad optimizer: 100.0%\n",
      "Training with Adadelta optimizer...\n",
      "Epoch 1, Loss: 0.9790827433268229, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 2, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 3, Loss: 0.2855045000712077, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 4, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 5, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 6, Loss: 0.7759160995483398, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 7, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 8, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 9, Loss: 0.0, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Epoch 10, Loss: 0.17711337407430014, Train Accuracy: 100.0%, Validation Accuracy: 100.0%\n",
      "Test Accuracy with Adadelta optimizer: 100.0%\n"
     ]
    }
   ],
   "source": [
    "#5.Compare performance with various optimization algorithms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "\n",
    "# Define paths to datasets\n",
    "training_path = \"/home/manichandana-sandhaboina/Downloads/train\"\n",
    "validation_path = \"/home/manichandana-sandhaboina/Downloads/validation\"\n",
    "testing_path = \"/home/manichandana-sandhaboina/Downloads/test\"\n",
    "\n",
    "# Define transforms for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Paths to datasets\n",
    "datasets = [\n",
    "    {'name': 'train', 'path': training_path},\n",
    "    {'name': 'validation', 'path': validation_path},\n",
    "    {'name': 'test', 'path': testing_path}\n",
    "]\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "data_loaders = {}\n",
    "for dataset in datasets:\n",
    "    images = [f for f in os.listdir(dataset['path']) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    data = []\n",
    "    labels = []\n",
    "    for img_name in images:\n",
    "        img_path = os.path.join(dataset['path'], img_name)\n",
    "        image = Image.open(img_path)\n",
    "        image = transform(image)\n",
    "        data.append(image)\n",
    "        labels.append(torch.tensor(0))  # Assuming all labels are 0 for simplicity\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    data_tensor = torch.stack(data)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "\n",
    "    # Create TensorDataset and DataLoader\n",
    "    data_loaders[dataset['name']] = DataLoader(TensorDataset(data_tensor, labels_tensor), batch_size=64, shuffle=dataset['name'] == 'train')\n",
    "\n",
    "# Neural network model\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(224 * 224 * 3, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# List of optimization algorithms to compare\n",
    "optimizers = [\n",
    "    {'name': 'SGD', 'optimizer': optim.SGD(net.parameters(), lr=0.01, momentum=0.9)},\n",
    "    {'name': 'Adam', 'optimizer': optim.Adam(net.parameters(), lr=0.001)},\n",
    "    {'name': 'RMSprop', 'optimizer': optim.RMSprop(net.parameters(), lr=0.001)},\n",
    "    {'name': 'Adagrad', 'optimizer': optim.Adagrad(net.parameters(), lr=0.01)},\n",
    "    {'name': 'Adadelta', 'optimizer': optim.Adadelta(net.parameters(), lr=1.0)}\n",
    "]\n",
    "\n",
    "# Training loop\n",
    "for optimizer_info in optimizers:\n",
    "    optimizer_name = optimizer_info['name']\n",
    "    optimizer = optimizer_info['optimizer']\n",
    "    print(f\"Training with {optimizer_name} optimizer...\")\n",
    "\n",
    "    # Reset model parameters\n",
    "    for module in net.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Dropout):\n",
    "            module.p = 0.5\n",
    "\n",
    "    # Train the network\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        net.train()  # Set the model to train mode\n",
    "        for dataset_name in ['train']:\n",
    "            for i, (inputs, labels) in enumerate(data_loaders[dataset_name], 0):\n",
    "                optimizer.zero_grad()\n",
    "                inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loaders['train']:\n",
    "                inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            train_accuracy = 100 * correct / total\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in data_loaders['validation']:\n",
    "                inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            validation_accuracy = 100 * correct / total\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Loss: {running_loss / len(data_loaders[\"train\"])}, Train Accuracy: {train_accuracy}%, Validation Accuracy: {validation_accuracy}%')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loaders['test']:\n",
    "            inputs = inputs.view(-1, 224 * 224 * 3)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        test_accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy with {optimizer_name} optimizer: {test_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40fd239-0e98-4e5b-aeac-bb7ddd7d7997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
